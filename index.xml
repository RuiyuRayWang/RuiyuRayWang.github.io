<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Karmotrine Dream</title>
    <link>/</link>
    <description>Recent content on Karmotrine Dream</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Jun 2022 20:52:38 +0800</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>Switch R Faster</title>
        <link>/post/switch-r-faster/</link>
        <pubDate>Wed, 15 Jun 2022 20:52:38 +0800</pubDate>
        
        <guid>/post/switch-r-faster/</guid>
        <description>Karmotrine Dream /post/switch-r-faster/ -&lt;p&gt;Previously I wrote about how to build multiple R versions on Ubuntu and how to switch versions.&lt;/p&gt;
/post/mult-r/
&lt;p&gt;It turns out that my &amp;ldquo;old school&amp;rdquo; move (&lt;code&gt;ln -s&lt;/code&gt;) is outdated. Now there are many handy tools to do the job for you.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rud.is/rswitch/&#34;&gt;RSwitch&lt;/a&gt; is based on macOS. I&amp;rsquo;m using it personally, simply because I used it before and don&amp;rsquo;t want to bother changing gear.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r-lib/rig&#34;&gt;rig&lt;/a&gt; supports multiple platforms and is probably the best choice for most users.&lt;/p&gt;
&lt;p&gt;Happy R swithing!&lt;/p&gt;
- /post/switch-r-faster/ - </description>
        </item>
    
    
    
        <item>
        <title>The Bitter Lesson</title>
        <link>/post/the_bitter_lesson/</link>
        <pubDate>Fri, 27 May 2022 09:21:08 +0800</pubDate>
        
        <guid>/post/the_bitter_lesson/</guid>
        <description>Karmotrine Dream /post/the_bitter_lesson/ -&lt;p&gt;Richard Sutton, the reknowned computer scientist, author of the AI bible: &amp;lsquo;Reinforcement Learning&amp;rsquo;, wrote this essay three years after Alphago beat Lee Sedol.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;../../figs/the_bitter_lesson.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34;&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here are some excerptions from the article:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. &amp;hellip; These researchers wanted methods based on human input to win and were disappointed when they did not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;A similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. &amp;hellip; In computer Go, as in computer chess, researchers&#39; initial effort was directed towards utilizing human understanding (so that less search was needed) and only much later was much greater success had by embracing search and learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. &amp;hellip; As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds worked&amp;mdash;they tried to put that knowledge in their systems&amp;mdash;but it proved ultimately counterproductive, and a colossal waste of researcher&amp;rsquo;s time, when, through Moore&amp;rsquo;s law, massive computation became available and a means was found to put it to good use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. &lt;strong&gt;We have to learn the bitter lesson that building in how we think we think does not work in the long run.&lt;/strong&gt; The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are &lt;em&gt;search&lt;/em&gt; and &lt;em&gt;learning&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The second general point to be learned from the bitter lesson is that &lt;strong&gt;the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds&lt;/strong&gt;, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. &lt;strong&gt;We want AI agents that can discover like we can, not which contain what we have discovered.&lt;/strong&gt; Building in our discoveries only makes it harder to see how the discovering process can be done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These points seem relevant not just to the fields of AI, but to other fields of science as well.&lt;/p&gt;
- /post/the_bitter_lesson/ - </description>
        </item>
    
    
    
        <item>
        <title>多重比较与P值矫正(一)：FWER</title>
        <link>/post/p_correction_fdr/</link>
        <pubDate>Sun, 27 Feb 2022 22:57:36 +0800</pubDate>
        
        <guid>/post/p_correction_fdr/</guid>
        <description>Karmotrine Dream /post/p_correction_fdr/ -&lt;p&gt;随着高通量技术逐渐发展完善，现代生物学的组学实验已经可以实现同时测量成百上千甚至上万个变量（variables）。如一次转录组实验可以得到10000-20000个基因的转录本丰度，蛋白组实验可以得到几千个有效蛋白肽段的丰度，磷酸化蛋白组可以得到~10000个磷酸化位点的肽段丰度。一组实验中我们往往会设置至少两个条件，而&lt;/p&gt;
- /post/p_correction_fdr/ - </description>
        </item>
    
    
    
        <item>
        <title>Tidy Data</title>
        <link>/post/tidy_data/</link>
        <pubDate>Mon, 21 Feb 2022 18:56:53 +0800</pubDate>
        
        <guid>/post/tidy_data/</guid>
        <description>Karmotrine Dream /post/tidy_data/ -&lt;figure&gt;&lt;img src=&#34;../../figs/tidydata.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Hadley Wickham提出的Tidy Data的概念方兴未艾，如今无论是R还是Python都能见到Tidy的身影。Tidydata的思维方式，将来必定会越来越深入人心。&lt;/p&gt;
&lt;p&gt;正如进入21世纪，计算机基本操作逐渐变为一项几乎必修的技能一样，将来十几年内，Tidydata对DataFrame的操作也一定会演变成一项必修技能。&lt;/p&gt;
&lt;p&gt;如果要开一个&lt;code&gt;计算生物学101&lt;/code&gt;的课程，&amp;ldquo;Tidy Data&amp;quot;这篇文章一定是必读清单的一篇文章。而且应该安排学生每学期花60个学时以上的时间上机实操锻炼Tidydata的操作。（我真是魔鬼）&lt;/p&gt;
- /post/tidy_data/ - </description>
        </item>
    
    
    
        <item>
        <title>一图看懂单细胞文库Reads的组成成分</title>
        <link>/post/10x_reads_composition/</link>
        <pubDate>Sun, 20 Feb 2022 10:55:43 +0800</pubDate>
        
        <guid>/post/10x_reads_composition/</guid>
        <description>Karmotrine Dream /post/10x_reads_composition/ -&lt;p&gt;这篇文章是前两篇技术文档的后续：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/post/10x_calc_saturation_1/&#34;&gt;10X CellRanger 中测序饱和度的定义与计算（一）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/post/10x_calc_saturation_2/&#34;&gt;10X CellRanger 中测序饱和度的定义与计算（二）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;前两篇文章成文过程中我意识到，单细胞测序文库从Raw Data到Deduplicated Reads，其实是一个non-trivial的流程。从fastq文件下机开始，不同来源的Reads需经过层层计算和筛选，最终只有少部分Reads能保留在表达矩阵（或counts table）中。这里用一张图总结，以帮助我们更好地理解单细胞文库的建库和上游分析（Upstream Analysis）。&lt;/p&gt;
&lt;hr&gt;
&lt;figure&gt;&lt;img src=&#34;../../figs/reads_composition.png&#34;/&gt;
&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;以&lt;a href=&#34;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&#34;&gt;ScRNAseq_smkpipe_at_Luolab&lt;/a&gt;的分析pipeline为例。&lt;br&gt;
这套流程主要针对使用了Barcode + UMI的技术。&lt;/p&gt;
&lt;p&gt;流程图读法：从下往上读，不同颜色代表不同分析阶段的Reads；画斜条纹阴影的线段表示当前步骤到下一步中被清除掉的Reads。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;黑色：Total Input Reads，一般我用的是测序下机后的Clean Reads（公司给的QC质控后的Cleandata）&lt;/li&gt;
&lt;li&gt;黑色-&amp;gt;紫色：&lt;code&gt;umi_tools whitelist&lt;/code&gt;, &lt;code&gt;wash_whitelist&lt;/code&gt;, &lt;code&gt;umi_tools extract&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;黑色阴影：Reads with Non-correctable Reads, invalid Barcode &amp;amp; UMI (discarded)&lt;/li&gt;
&lt;li&gt;紫色：Barcode有效的Reads（Reads with valid Barcode &amp;amp; UMI）&lt;/li&gt;
&lt;li&gt;紫色-&amp;gt;黄色：&lt;code&gt;STAR&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;紫色阴影：Reads NOT Unique Mapped to Genome (discarded)&lt;/li&gt;
&lt;li&gt;黄色：Unique Mapped Reads，在10X中也叫Confidently Mapped Reads（Unique Mapped, valid Barcode &amp;amp; UMI Reads）&lt;/li&gt;
&lt;li&gt;黄色-&amp;gt;绿色：&lt;code&gt;featureCounts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;黄色阴影：Unique Mapped, valid Barcode &amp;amp; UMI but NOT assigned to feature&lt;/li&gt;
&lt;li&gt;绿色：Effective Reads，即Unique Mapped, valid Barcode &amp;amp; UMI, feature assigned Reads&lt;/li&gt;
&lt;li&gt;绿色-&amp;gt;蓝色：&lt;code&gt;umi_tools count&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;蓝色阴影：Reads Collapsed due to UMI deduplication&lt;/li&gt;
&lt;li&gt;蓝色：Deduplicated Reads，即最终用于生成表达矩阵的对Reads计数的结果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了这张图，我们可以做一个简单直观的推论：如果想提高单细胞测序的技术质量，则必须尽可能减少阴影部分的Reads在总文库中的比例。&lt;/p&gt;
&lt;p&gt;仔细思考实验流程，会发现每个阴影部分会受不同因素的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;黑色阴影：主要影响因素之一是Barcode &amp;amp; UMI序列质量，即TSO引物质量；也有可能受TSO concatamer、RNA降解因素影响&lt;/li&gt;
&lt;li&gt;紫色阴影：主要影响因素是样本制备质量，即细胞活率、异源RNA污染等&lt;/li&gt;
&lt;li&gt;黄色阴影：主要影响因素是生物学样本内非mRNA成分的比例，如premRNA等，可能与样本本身生物学性质有关&lt;/li&gt;
&lt;li&gt;蓝色阴影：主要与扩增循环数有关，该部分Reads过多可能是因为扩增循环数过高&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“降低阴影部分比例”可以作为单细胞测序实验的一项指导原则，帮助我们进行实验条件的摸索，也可以作为单细胞技术开发的一项指导原则，帮助我们迭代优化试剂组成。&lt;/p&gt;
- /post/10x_reads_composition/ - </description>
        </item>
    
    
    
        <item>
        <title>Hugo：启用Mathjax实现数学公式的编辑与显示</title>
        <link>/post/deploy_mathjax/</link>
        <pubDate>Sat, 12 Feb 2022 21:17:37 +0800</pubDate>
        
        <guid>/post/deploy_mathjax/</guid>
        <description>Karmotrine Dream /post/deploy_mathjax/ -&lt;p&gt;The HUGO official document has a nice blog about &lt;a href=&#34;https://bwaycer.github.io/hugo_tutorial.hugo/tutorials/mathjax/&#34;&gt;MathJax Support&lt;/a&gt;.
Additionally, a blog written by Geoff Ruddock has a more concise solution: &lt;a href=&#34;https://geoffruddock.com/math-typesetting-in-hugo/&#34;&gt;Render LaTeX math expressions in Hugo with MathJax 3&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;tldr&#34;&gt;TL;DR&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Create a file in your theme directory &lt;code&gt;layouts/partials/mathjax_support.html&lt;/code&gt; as the following:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;script&amp;gt;
  MathJax = {
    tex: {
      inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\\(&#39;, &#39;\\)&#39;]],
      displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;, &#39;\\]&#39;]],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;]
    }
  };

  window.addEventListener(&#39;load&#39;, (event) =&amp;gt; {
      document.querySelectorAll(&amp;quot;mjx-container&amp;quot;).forEach(function(x){
        x.parentElement.classList += &#39;has-jax&#39;})
    });

&amp;lt;/script&amp;gt;
&amp;lt;script src=&amp;quot;https://polyfill.io/v3/polyfill.min.js?features=es6&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script type=&amp;quot;text/javascript&amp;quot; id=&amp;quot;MathJax-script&amp;quot; async
  src=&amp;quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Next, open the file &lt;code&gt;layouts/partials/header.html&lt;/code&gt; and add the following line just before the closing &lt;!-- raw HTML omitted --&gt; tag:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{{ if .Params.mathjax }}{{ partial &amp;quot;mathjax_support.html&amp;quot; . }}{{ end }}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Then, add the following lines to your CSS file. You may need to tinker with the contents here depending on your theme, these are just the settings which worked for me.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;code.has-jax {font: inherit;
              font-size: 100%;
              background: inherit;
              border: inherit;
              color: #515151;}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To modify css, refer to &lt;a href=&#34;https://www.banjocode.com/post/hugo/custom-css/&#34;&gt;Add Custom CSS Or Javascript To Your Hugo Site&lt;/a&gt;。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Finally, add &lt;code&gt;mathjax: true&lt;/code&gt; to the YAML frontmatter of any pages containing math markup. Alternatively, you could omit the outer &lt;code&gt;{{ if .Params.mathjax }} … {{ end }}&lt;/code&gt; conditional above to load the library automatically on all pages. However given that this library is quite heavy (it’s consistently the asset that Google PageSpeed Insights complains the most about) and that only &amp;lt;20% of my blog posts contain math at all, this is worth the extra effort for me.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;todo&#34;&gt;TODO&lt;/h1&gt;
&lt;p&gt;走完上面的流程后还是发现MathJax中&lt;code&gt;\color&lt;/code&gt;和&lt;code&gt;\textcolor&lt;/code&gt;不能修改字体颜色。&lt;br&gt;
custom.css似乎也没有正确配置。&lt;/p&gt;
- /post/deploy_mathjax/ - </description>
        </item>
    
    
    
        <item>
        <title>10X CellRanger 中测序饱和度的定义与计算（一）</title>
        <link>/post/10x_calc_saturation_1/</link>
        <pubDate>Tue, 08 Feb 2022 22:05:51 +0800</pubDate>
        
        <guid>/post/10x_calc_saturation_1/</guid>
        <description>Karmotrine Dream /post/10x_calc_saturation_1/ -&lt;p&gt;最近为了搭建单细胞测序上游分析的Pipeline，正在研究怎样对分析结果做质控（Pipeline详情移步&lt;a href=&#34;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&#34;&gt;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&lt;/a&gt;）。参考了10X CellRanger官方网站中描述的对测序饱和度的定义和计算方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kb.10xgenomics.com/hc/en-us/articles/115005062366-What-is-sequencing-saturation-&#34;&gt;10X 官方文档：什么是测序饱和度？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kb.10xgenomics.com/hc/en-us/articles/115003646912-How-is-sequencing-saturation-calculated-&#34;&gt;10X 官方文档：测序饱和度怎样计算？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将用两篇文章记录这部分内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：本系列文章的参考来源均为10X官方公开的信息。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这篇文章是本系列文章的第一篇，将对相关概念和计算公式的定义进行解读。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一测序饱和度-和-文库复杂度&#34;&gt;一、&lt;code&gt;测序饱和度&lt;/code&gt; 和 &lt;code&gt;文库复杂度&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;关于测序饱和度的定义，引用10X文档中的说明：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is sequencing saturation?&lt;br&gt;
&lt;strong&gt;Answer:&lt;/strong&gt; Sequencing saturation is a measure of &lt;em&gt;&lt;strong&gt;the fraction of library complexity&lt;/strong&gt;&lt;/em&gt; that was sequenced in a given experiment. The &lt;em&gt;&lt;strong&gt;inverse&lt;/strong&gt;&lt;/em&gt; of the sequencing saturation can be interpreted as the number of additional reads it would take to detect a new transcript.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这两句话言简而意赅，点明了测序饱和度(sequencing saturation)与文库复杂度(library complexity)之间的关系。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gatk.broadinstitute.org/hc/en-us/articles/360037591931-EstimateLibraryComplexity-Picard-&#34;&gt;library complexity (文库复杂度)&lt;/a&gt;，指的是一个测序文库中非冗余的DNA片段的数量（the number of unique DNA fragments present in a given library）。&lt;br&gt;
&lt;strong&gt;&lt;code&gt;测序饱和度&lt;/code&gt;表征了当前的测序数据中测出了多少&lt;code&gt;文库复杂度&lt;/code&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;从逆命题的角度来看待这个问题：发明一个新名词&lt;code&gt;“测序不饱和度”&lt;/code&gt;，则&lt;code&gt;“测序不饱和度”&lt;/code&gt;可以定义为 &lt;em&gt;再多测序多少个Reads就可以检测到一个全新的转录本&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;。&lt;br&gt;
换句话说，&lt;strong&gt;如果我们只需要再测很少的Reads就可以得到一个全新的转录本，说明这个文库“测序不饱和”；反之，如果需要再测很多Reads才能得到一个全新的转录本，说明这个文库“测序饱和”。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其实我们还可以从&lt;strong&gt;冗余信息&lt;/strong&gt;的角度来理解这个问题：&lt;br&gt;
对于一个文库，&lt;strong&gt;如果仅再测很少Reads就检测到了全新的转录本，说明当前数据还远未挖掘出文库中的大部分信息，数据的信息冗余程度比较低，测序不饱和；&lt;/strong&gt;&lt;br&gt;
而反过来，&lt;strong&gt;如果需要再测很多Reads才能检测到全新的转录本，说明当前数据对文库中包含的信息挖掘得比较充分，数据的信息冗余程度比较高，测序接近饱和。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以本质上，测序饱和度可以间接地由&lt;strong&gt;信息冗余程度&lt;/strong&gt;进行度量。&lt;br&gt;
数据信息冗余度越高，测序饱和度越高；&lt;br&gt;
数据信息冗余度越低，测序饱和度越低。&lt;/p&gt;
&lt;p&gt;从实际角度出发，我们有时候&lt;strong&gt;并不希望信息冗余度&lt;/strong&gt;太高。因为冗余信息越多，浪费的资源越多。所以有些研究不会将 测序饱和度 的目标阈值设定到接近饱和（&amp;gt;90%），而是测到70%左右就收工了。&lt;br&gt;
在受经费条件等因素限制的情况下这是一种合理策略。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二影响测序饱和度的因素&#34;&gt;二、影响测序饱和度的因素&lt;/h3&gt;
&lt;p&gt;在单细胞测序中，测序饱和度主要由两个因素决定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;文库复杂度(Biological Complxity)：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Different cell types will have different amounts of RNA and thus will differ in the total number of different transcripts in the final library (also known as library complexity). The figure below illustrates the median number of genes recovered from different cell types. As sequencing depth increases, more genes are detected, but this reaches saturation at different sequencing depths depending on cell type.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;文库复杂度一般由生物学样本的复杂程度决定。而生物学样本的复杂度又由多个因素，如细胞异质性(Cell Heterogeneity)、转录组RNA含量、细胞质量、细胞数等共同决定。&lt;del&gt;越是简单均一的系统，生物复杂程度越低。&lt;/del&gt;&lt;br&gt;
下图中，原代分离的PBMC(外周血单核细胞)和E18小鼠神经元细胞RNA含量较低（？）&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，因此迅速达到测序饱和；而培养的永生细胞3T3和HEK293T的RNA含量比较高（？）&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，测序不容易达到饱和。同样作为永生细胞，3T3细胞系的转录组比HEK293T细胞系的转录组复杂，因此相比之下不容易测序饱和。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;测序深度(Sequencing Depth)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sequencing depth also affects sequencing saturation; generally, the more sequencing reads, the more additional unique transcripts you can detect. However, this is limited by the library complexity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;即每个文库实际测到的Reads数。在控制其他条件不变的情况下，一般来讲测序深度越高测序饱和度越高，直观上这很容易理解。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kb.10xgenomics.com/hc/article_attachments/115016412246/SingleCell_GeneRecovery_by_SeqDepth_combined.png#middle&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;上面的内容可以总结在一个公式中：&lt;/p&gt;
&lt;p&gt;$$测序饱和度\propto\frac{测序深度}{文库复杂度}$$&lt;/p&gt;
&lt;p&gt;如果一个样本的生物学成分比较复杂，那送样时就可以提前计划适当提高测序深度；如果测完发现测序饱和度还是不够，那就对文库进行加测（提高测序深度）。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;三测序饱和度的计算&#34;&gt;三、测序饱和度的计算&lt;/h3&gt;
&lt;p&gt;10X官方文档给出了测序饱和度计算公式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$Sequencing\ Saturation = 1 - \frac{n_{deduped\ reads}}{n_{reads}}$$&lt;br&gt;
where&lt;br&gt;
$n_{deduped\ reads}$ = Number of unique (valid cell-barcode, valid UMI, gene) combinations among confidently mapped reads&lt;br&gt;
$n_{reads}$ = Total number of confidently mapped, valid cell-barcode, valid UMI reads&lt;br&gt;
.&lt;br&gt;
Note that the numerator of the fraction is $n_{deduped\ reads}$, not the non-unique reads that are mentioned in the definition. $n_{deduped\ reads}$ is a degree of uniqueness, not a degree of duplication/saturation. Therefore we take the complement of $n_{deduped\ reads}/n_{reads}$ to measure saturation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;即&lt;/p&gt;
&lt;p&gt;$$测序饱和度=1-\frac{去重后Reads数}{总Reads数}=\frac{重复Reads数}{总Reads数}$$&lt;/p&gt;
&lt;p&gt;注意重复Reads是通过Barcode和UMI定义的。两个Reads出现同样的Barcode + UMI组合，就将其中一个定义为重复；三个Reads出现同样的Barcode + UMI组合，其中两个定义为重复，剩下一个定义为去重后Read。&lt;/p&gt;
&lt;p&gt;从这个公式也可以看出，重复Reads数越高，冗余信息越多，测序饱和度也越高。&lt;/p&gt;
&lt;p&gt;该公式只计算了一个数值，代表某次测序fastq文件整个数据的测序饱和度。如果要实现10X CellRanger输出web_summary的中饱和度曲线的绘制，感觉还是比较麻烦的。理论上，需要对fastq文件做不同水平的降采样，再分别算测序饱和度。&lt;br&gt;
目前还不清楚CellRanger是怎么实现一次alignment过程中输出不同测序深度下的测序饱和度的。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;全新的转录本，在10X官方文档的语境下，指的是一个全新的 Barcode + UMI 组合（i.e. a previously unobserved, unique combination of valid cell-barcode and valid UMI）&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;直觉上这并不 make sense。原代细胞有较高的细胞异质性，如果其他条件相同，理应更难达到测序饱和。猜测也许是因为原代细胞分离下来细胞状态不如永生细胞，因此导致文库质量不佳。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
- /post/10x_calc_saturation_1/ - </description>
        </item>
    
    
    
        <item>
        <title>10X CellRanger 中测序饱和度的定义与计算（二）</title>
        <link>/post/10x_calc_saturation_2/</link>
        <pubDate>Tue, 08 Feb 2022 22:05:51 +0800</pubDate>
        
        <guid>/post/10x_calc_saturation_2/</guid>
        <description>Karmotrine Dream /post/10x_calc_saturation_2/ -&lt;p&gt;最近为了搭建单细胞测序上游分析的Pipeline，正在研究怎样对分析结果做质控（Pipeline详情移步&lt;a href=&#34;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&#34;&gt;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&lt;/a&gt;）。参考了10X CellRanger官方网站中描述的对测序饱和度的定义和计算方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kb.10xgenomics.com/hc/en-us/articles/115005062366-What-is-sequencing-saturation-&#34;&gt;10X 官方文档：什么是测序饱和度？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kb.10xgenomics.com/hc/en-us/articles/115003646912-How-is-sequencing-saturation-calculated-&#34;&gt;10X 官方文档：测序饱和度怎样计算？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将用两篇文章记录这部分内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：本系列文章的参考来源均为10X官方公开的信息。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这篇文章是本系列文章的第二篇，将利用10X官方文档中的示例数据来展示测序饱和度计算的代码复现。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;下载数据：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Uses dataset at &lt;a href=&#34;https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_1k_v3&#34;&gt;https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_1k_v3&lt;/a&gt;, where the web summary report gives a sequencing saturation value of 0.7085123.​&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_1k_v3/pbmc_1k_v3_web_summary.html&#34;&gt;web_summary.html&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;计算公式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The sequencing saturation calculation below matches the 0.7085123 sequencing saturation given in the web summary report.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;unique_confidently_mapped_reads = 10,196,940
duplicate_reads = 24,785,461

x = 1 - (unique_confidently_mapped_reads/(unique_confidently_mapped_reads + duplicate_reads))
x = 1 - (10,196,940/(10,196,940 + 24,785,461))
x = 1 - (10,196,940/34,982,401)
x = 1 - 0.2148771
x = 0.70851229
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;p&gt;&lt;code&gt;web_summary.html&lt;/code&gt;中并没有直接给出&lt;code&gt;unique_configdently_mapped_reads&lt;/code&gt;和&lt;code&gt;duplicate_reads&lt;/code&gt;具体是多少。实际上这两个参数都是可以从bam中抓取的。具体抓取方法在官方文档中也已经给出。我这里介绍另一种有点hacky的方法，可以从结果反推算出&lt;code&gt;unique_configdently_mapped_reads&lt;/code&gt;是多少。从这个方法我们可以一窥10X输出的文件结构，也可以加深我们对数据含义的理解。&lt;/p&gt;
&lt;p&gt;下载&lt;code&gt;Feature / cell matrix (raw)&lt;/code&gt;，得到文件&lt;code&gt;pbmc_1k_v3_raw_feature_bc_matrix.tar.gz&lt;/code&gt;，将其解压：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ tar -xvzf pbmc_1k_v3_raw_feature_bc_matrix.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;得到&lt;code&gt;raw_feature_bc_matrix/&lt;/code&gt;文件夹，进入后将其中&lt;code&gt;matrix.mtx.gz&lt;/code&gt;文件进一步解压出来：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ cd raw_feature_bc_matrix
$ gunzip -k matrix.mtx.gz
$ head matrix.mtx
%%MatrixMarket matrix coordinate integer general
%metadata_json: {&amp;quot;format_version&amp;quot;: 2, &amp;quot;software_version&amp;quot;: &amp;quot;3.0.0&amp;quot;}
33538 6794880 3394796
1815 9 1
4027 9 1
6626 9 1
17060 9 1
18003 9 1
21000 9 1
30469 9 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个文件起始有两行header，随后是一个有三列的表，每列由空格分隔。这个表相当于一个DataFrame。&lt;br&gt;
第一列表示基因，第二列表示细胞，第三列表示该基因在该细胞中Read的count数（和&lt;code&gt;umi_tools count&lt;/code&gt;的输出一模一样）。&lt;/p&gt;
&lt;p&gt;将前两行的header去掉，把表读到R里：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ tail +3 matrix.mtx &amp;gt; tmp
$ mv tmp matrix.mtx
$ R
&amp;gt; raw &amp;lt;- read.table(&amp;quot;matrix.mtx&amp;quot;, sep=&amp;quot; &amp;quot;, header=T)
&amp;gt; head(raw)
  X33538 X6794880 X3394796
1   1815        9        1
2   4027        9        1
3   6626        9        1
4  17060        9        1
5  18003        9        1
6  21000        9        1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对第三列求和，这本质上相当于deduplicate之后数据里剩下的reads数：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; sum(raw$X3394796)
[1] 10196940
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;会发现这个数值就是&lt;code&gt;unique_confidently_mapped_reads&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;上面介绍的方法相当于是“逆向工程”反推出&lt;code&gt;unique_confidently_mapped_reads&lt;/code&gt;是多少，是一个比较hacky的办法。&lt;/p&gt;
&lt;p&gt;10X官方文档中是正儿八经从bam文件正向算的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The unique_confidently_mapped_reads are the reads with xf tag value 25, as described on &lt;a href=&#34;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/bam&#34;&gt;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/bam&lt;/a&gt;. The bits are 1, 8, 16. You can obtain theunique_confidently_mapped_reads by parsing the BAM like so.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;samtools view pbmc_1k_v3_possorted_genome_bam.bam | grep &#39;xf:i:25&#39; | wc -l
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到，CellRanger在做Alignment时会给&lt;code&gt;unique_confidently_mapped_reads&lt;/code&gt;打上一个tag&lt;code&gt;xf=25&lt;/code&gt;。这里是用samtools在bam文件中抓取这些tag来计算&lt;code&gt;unique_confidently_mapped_reads&lt;/code&gt;的。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;至于&lt;code&gt;duplicate_reads&lt;/code&gt;，在10X给出的数据中似乎只能从bam文件抓取。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The duplicate_reads are those marked with the sam flag 0x400 as explained on &lt;a href=&#34;https://broadinstitute.github.io/picard/explain-flags.html&#34;&gt;https://broadinstitute.github.io/picard/explain-flags.html&lt;/a&gt; and can be obtained by running samtools flagstat.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;samtools flagstat pbmc_1k_v3_possorted_genome_bam.bam
76920923 + 0 in total (QC-passed reads + QC-failed reads)
10319036 + 0 secondary
0 + 0 supplementary
24785461 + 0 duplicates
73840063 + 0 mapped (95.99% : N/A)
...
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;All of these samflag 0x400 reads have an xf tag value of 17, which consist of bits 1 and 16. This also means these reads do not have the xf bit of 8, which mark representative reads from a group of duplicates. The converse isn&amp;rsquo;t true though. The xf17 consist mostly of samflag 0x400 duplicate reads but also of samflag nonduplicate reads.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;而如果使用&lt;a href=&#34;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&#34;&gt;&lt;code&gt;ScRNAseq_smkpipe_at_Luolab&lt;/code&gt;&lt;/a&gt;的protocol，则可以从&lt;code&gt;umi_tools&lt;/code&gt;或者&lt;code&gt;featureCounts&lt;/code&gt;输出的log文件抓取。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;有了&lt;code&gt;unique_configdently_mapped_reads&lt;/code&gt;和&lt;code&gt;duplicate_reads&lt;/code&gt;，测序饱和度就可以直接计算了：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;unique_confidently_mapped_reads = 10,196,940
duplicate_reads = 24,785,461

x = 1 - (unique_confidently_mapped_reads/(unique_confidently_mapped_reads + duplicate_reads))
x = 1 - (10,196,940/(10,196,940 + 24,785,461))
x = 1 - (10,196,940/34,982,401)
x = 1 - 0.2148771
x = 0.70851229
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里只是计算了fastq文件全部分析完后的&lt;code&gt;测序饱和度&lt;/code&gt;。如果要绘制测序饱和度曲线，则要在不同降采样水平的fastq上逐个计算。&lt;br&gt;
如果利用10X给出的从bam文件抓取信息的方法，我们就可以在bam上做随机降采样，然后抓取数值，计算每个测序深度水平的&lt;code&gt;测序饱和度&lt;/code&gt;了。&lt;/p&gt;
- /post/10x_calc_saturation_2/ - </description>
        </item>
    
    
    
        <item>
        <title>R SCENIC：安装与部署</title>
        <link>/post/install-r-scenic/</link>
        <pubDate>Wed, 22 Dec 2021 12:32:17 +0800</pubDate>
        
        <guid>/post/install-r-scenic/</guid>
        <description>Karmotrine Dream /post/install-r-scenic/ -&lt;h1 id=&#34;tldr&#34;&gt;TL;DR&lt;/h1&gt;
&lt;p&gt;First consulted &lt;a href=&#34;http://htmlpreview.github.io/?https://github.com/aertslab/SCENIC/blob/master/inst/doc/SCENIC_Setup.html&#34;&gt;this page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have &lt;code&gt;R-4.0.5&lt;/code&gt; with &lt;code&gt;Bioconductor 3.12(4.0)&lt;/code&gt;, use the latest scripts for installation.&lt;/p&gt;
&lt;p&gt;In Rstudio:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; if (!requireNamespace(&amp;quot;BiocManager&amp;quot;, quietly = TRUE)) install.packages(&amp;quot;BiocManager&amp;quot;)
&amp;gt; BiocManager::version()
&amp;gt; # If your bioconductor version is previous to 4.0, see the section bellow
&amp;gt; 
&amp;gt; ## Required
&amp;gt; BiocManager::install(c(&amp;quot;AUCell&amp;quot;, &amp;quot;RcisTarget&amp;quot;))
&amp;gt; BiocManager::install(c(&amp;quot;GENIE3&amp;quot;)) # Optional. Can be replaced by GRNBoost
&amp;gt; 
&amp;gt; ## Optional (but highly recommended):
&amp;gt; # To score the network on cells (i.e. run AUCell):
&amp;gt; BiocManager::install(c(&amp;quot;zoo&amp;quot;, &amp;quot;mixtools&amp;quot;, &amp;quot;rbokeh&amp;quot;))
&amp;gt; # For various visualizations and perform t-SNEs:
&amp;gt; BiocManager::install(c(&amp;quot;DT&amp;quot;, &amp;quot;NMF&amp;quot;, &amp;quot;ComplexHeatmap&amp;quot;, &amp;quot;R2HTML&amp;quot;, &amp;quot;Rtsne&amp;quot;))
&amp;gt; # To support paralell execution (not available in Windows):
&amp;gt; BiocManager::install(c(&amp;quot;doMC&amp;quot;, &amp;quot;doRNG&amp;quot;))
&amp;gt; &amp;gt; # To export/visualize in http://scope.aertslab.org
&amp;gt; if (!requireNamespace(&amp;quot;devtools&amp;quot;, quietly = TRUE)) install.packages(&amp;quot;devtools&amp;quot;)
&amp;gt; devtools::install_github(&amp;quot;aertslab/SCopeLoomR&amp;quot;, build_vignettes = TRUE)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These worked fine.&lt;/p&gt;
&lt;p&gt;Then, when installing SCENIC using &lt;code&gt;devtools&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; if (!requireNamespace(&amp;quot;devtools&amp;quot;, quietly = TRUE)) install.packages(&amp;quot;devtools&amp;quot;)
&amp;gt; devtools::install_github(&amp;quot;aertslab/SCENIC&amp;quot;) 
Downloading GitHub repo aertslab/SCENIC@HEAD
Error in utils::download.file(url, path, method = method, quiet = quiet,  : 
  download from &#39;https://api.github.com/repos/aertslab/SCENIC/tarball/HEAD&#39; failed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Tried several times with no success.&lt;/p&gt;
&lt;p&gt;There are some discussions on this behavior with &lt;code&gt;devtools&lt;/code&gt; and &lt;code&gt;remotes&lt;/code&gt;, for example this &lt;a href=&#34;https://github.com/r-lib/remotes/issues/130&#34;&gt;issue&lt;/a&gt;, but I find it un-straightforward.&lt;/p&gt;
&lt;p&gt;Instead, I decided to switch to SCENIC(R) github page and install directly from there.
First tried directly cloning the repo:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ git clone https://github.com/aertslab/SCENIC.git
Cloning into &#39;SCENIC&#39;...
fatal: unable to access &#39;https://github.com/aertslab/SCENIC.git/&#39;: Could not resolve host: github.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This also failed!&lt;/p&gt;
&lt;p&gt;Then tried &lt;code&gt;Download ZIP&lt;/code&gt; to local and unzip &lt;code&gt;SCENIC-master.zip&lt;/code&gt;. Create a new project in Rstudio from the directory containing SCENIC build and hit the &lt;code&gt;Install and Restart&lt;/code&gt; button.&lt;/p&gt;
&lt;p&gt;This also failed because of an error of the sort &amp;ldquo;Dependency &amp;lsquo;dynamicTreeCut&amp;rsquo; not available&amp;rdquo;, but this won&amp;rsquo;t be very difficult to solve.&lt;/p&gt;
&lt;p&gt;Install &lt;code&gt;dynamicTreeCut&lt;/code&gt; manually:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;dynamicTreeCut&amp;quot;)
Warning in install.packages :
  unable to access index for repository https://cran.rstudio.com/src/contrib:
  cannot open URL &#39;https://cran.rstudio.com/src/contrib/PACKAGES&#39;
Installing package into ‘/home/luolab/R/x86_64-pc-linux-gnu-library/4.0’
(as ‘lib’ is unspecified)
Warning in install.packages :
  unable to access index for repository https://cran.rstudio.com/src/contrib:
  cannot open URL &#39;https://cran.rstudio.com/src/contrib/PACKAGES&#39;
Warning in install.packages :
  package ‘dynamicTreeCut’ is not available for this version of R

A version of this package for your version of R might be available elsewhere,
see the ideas at
https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To &lt;a href=&#34;https://community.rstudio.com/t/install-packages-unable-to-access-index-for-repository-try-disabling-secure-download-method-for-http/16578&#34;&gt;fix this&lt;/a&gt;, in Rstudio go to &lt;code&gt;Tools&lt;/code&gt; &amp;gt; &lt;code&gt;Global Options&lt;/code&gt; &amp;gt; &lt;code&gt;Packages&lt;/code&gt; and &lt;strong&gt;Uncheck&lt;/strong&gt; &lt;code&gt;Use secure download method for HTTP&lt;/code&gt;. Then:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;dynamicTreeCut&amp;quot;)
Installing package into ‘/home/luolab/R/x86_64-pc-linux-gnu-library/4.0’
(as ‘lib’ is unspecified)
trying URL &#39;http://cran.rstudio.com/src/contrib/dynamicTreeCut_1.63-1.tar.gz&#39;
Content type &#39;application/x-gzip&#39; length 24027 bytes (23 KB)
==================================================
downloaded 23 KB

* installing *source* package ‘dynamicTreeCut’ ...
** package ‘dynamicTreeCut’ successfully unpacked and MD5 sums checked
** using staged installation
** R
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (dynamicTreeCut)

The downloaded source packages are in
	‘/tmp/RtmpgfV9Bj/downloaded_packages’
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Success!&lt;/p&gt;
&lt;p&gt;Now &lt;code&gt;Install and Restart&lt;/code&gt; SCENIC again and everything worked.&lt;/p&gt;
&lt;p&gt;Next job is to import &lt;code&gt;pySCENIC&lt;/code&gt; results to R &lt;code&gt;SCENIC&lt;/code&gt;, consult the followings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/aertslab/pySCENIC/issues/180&#34;&gt;Github issue discussion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rawcdn.githack.com/aertslab/SCENIC/0a4c96ed8d930edd8868f07428090f9dae264705/inst/doc/importing_pySCENIC.html&#34;&gt;Vignette&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ps&#34;&gt;P.S.&lt;/h2&gt;
&lt;p&gt;When trying to push this post to github remote, failed again&amp;hellip;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ git push -u --all
fatal: unable to access &#39;https://github.com/RuiyuRayWang/blog.git/&#39;: Could not resolve host: github.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Looks like we&amp;rsquo;re having a bad day with github server&amp;hellip;&lt;/p&gt;
- /post/install-r-scenic/ - </description>
        </item>
    
    
    
        <item>
        <title>Working with pySCENIC</title>
        <link>/post/using-scenic/</link>
        <pubDate>Wed, 15 Dec 2021 23:07:11 +0800</pubDate>
        
        <guid>/post/using-scenic/</guid>
        <description>Karmotrine Dream /post/using-scenic/ -- /post/using-scenic/ - </description>
        </item>
    
    
  </channel>
</rss> 