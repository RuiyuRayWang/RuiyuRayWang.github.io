<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Karmotrine Dream</title>
    <link>/</link>
    <description>Recent content on Karmotrine Dream</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jun 2022 16:07:43 +0800</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>Debug: Installation of celda failed</title>
        <link>/post/debug-celda-magick-install/</link>
        <pubDate>Mon, 20 Jun 2022 16:07:43 +0800</pubDate>
        
        <guid>/post/debug-celda-magick-install/</guid>
        <description>Karmotrine Dream /post/debug-celda-magick-install/ -&lt;p&gt;While installing celda via bioconductor:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;BiocManager::install(&#39;celda&#39;)

...
...

Error: package or namespace load failed for ‘magick’ in dyn.load(file, DLLpath = DLLpath, ...):
 unable to load shared object &#39;/home/luolab/R/x86_64-pc-linux-gnu-library/4.2/magick/libs/magick.so&#39;:
  ... no such file or directory
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I&amp;rsquo;m running on Ubuntu 20.04 LTS.&lt;/p&gt;
&lt;p&gt;Installing imagemagick &lt;code&gt;Magick++&lt;/code&gt; library resolved the issue. See &lt;a href=&#34;https://github.com/ropensci/magick&#34;&gt;https://github.com/ropensci/magick&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ sudo apt-get install -y libmagick++-dev
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Everything works fine
BiocManager::install(&#39;celda&#39;)
&lt;/code&gt;&lt;/pre&gt;- /post/debug-celda-magick-install/ - </description>
        </item>
    
    
    
        <item>
        <title>Pytorch: End of Agony</title>
        <link>/post/pytorch-end-of-agony/</link>
        <pubDate>Sat, 18 Jun 2022 15:44:29 +0800</pubDate>
        
        <guid>/post/pytorch-end-of-agony/</guid>
        <description>Karmotrine Dream /post/pytorch-end-of-agony/ -&lt;p&gt;The agonizing days of building everything from scratch (NVIDIA driver, CUDA, cuDNN) is over.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t have to spend time on&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/cuda/index.html&#34;&gt;&lt;del&gt;CUDA documentaion&lt;/del&gt;&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;or building&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ruiyuraywang.github.io/post/build-tensorflow/&#34;&gt;&lt;del&gt;Tensorflow：多版本部署、系统配置与版本切换&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, use PyTorch, and you will be amazed at ease of building a GPU capable meachine learning model with neural network.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://amytabb.com/til/2020/10/05/cudnn-pytorch/&#34;&gt;The PyTorch binaries include the CUDA and cuDNN libraries.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://discuss.pytorch.org/t/how-to-check-if-torch-uses-cudnn/21933&#34;&gt;How to check if torch uses cuDNN&lt;/a&gt;&lt;/p&gt;
- /post/pytorch-end-of-agony/ - </description>
        </item>
    
    
    
        <item>
        <title>Fixing a Bug in R After Ubuntu Upgrade</title>
        <link>/post/fixing-a-bug-in-r-after-ubuntu-upgrade/</link>
        <pubDate>Fri, 17 Jun 2022 22:01:10 +0800</pubDate>
        
        <guid>/post/fixing-a-bug-in-r-after-ubuntu-upgrade/</guid>
        <description>Karmotrine Dream /post/fixing-a-bug-in-r-after-ubuntu-upgrade/ -&lt;p&gt;Everybody knows the painstaking experience of system update, expecially when it comes to reconfiguring the environments and re-installing previously installed packages. Even with package management softwares like conda, bad things could still happen.&lt;/p&gt;
&lt;p&gt;Recently I updated Ubuntu from 18.04 LTS to 20.04 LTS. At the same time I also decided to install the latest R 4.2.0 since there are some new features worth the bother.&lt;/p&gt;
&lt;p&gt;I use &lt;code&gt;rig&lt;/code&gt; to install R 4.2.0, and here comes the trouble.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(base) luolab@luolab-Z10PE-D16-WS:~$ rig add 4.2.0
[INFO] Running `sudo` for adding new R versions. This might need your password.
[INFO] Downloading https://cdn.rstudio.com/r/ubuntu-2004/pkgs/r-4.2.0_1_amd64.deb -&amp;gt;
    /tmp/rig/r-4.2.0_1_amd64.deb
[INFO] Running apt-get update
--nnn-- Start of apt-get output -------------------
Hit:1 https://dl.google.com/linux/chrome/deb stable InRelease
Hit:2 https://download.docker.com/linux/ubuntu bionic InRelease                
Hit:3 http://cn.archive.ubuntu.com/ubuntu focal InRelease                      
Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
Get:5 http://cn.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
Get:6 http://security.ubuntu.com/ubuntu focal-security/main amd64 DEP-11 Metadata [40.7 kB]
Get:7 http://cn.archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]
Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 DEP-11 Metadata [66.6 kB]
Get:9 http://cn.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1,924 kB]
Get:10 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 DEP-11 Metadata [2,464 B]
Get:11 http://cn.archive.ubuntu.com/ubuntu focal-updates/main i386 Packages [678 kB]
Get:12 http://cn.archive.ubuntu.com/ubuntu focal-updates/main amd64 DEP-11 Metadata [278 kB]
Get:13 http://cn.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [931 kB]
Get:14 http://cn.archive.ubuntu.com/ubuntu focal-updates/universe i386 Packages [684 kB]
Get:15 http://cn.archive.ubuntu.com/ubuntu focal-updates/universe amd64 DEP-11 Metadata [391 kB]
Get:16 http://cn.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 DEP-11 Metadata [944 B]
Get:17 http://cn.archive.ubuntu.com/ubuntu focal-backports/main amd64 DEP-11 Metadata [7,992 B]
Get:18 http://cn.archive.ubuntu.com/ubuntu focal-backports/universe amd64 DEP-11 Metadata [30.5 kB]
Fetched 5,370 kB in 8s (668 kB/s)                                              
Reading package lists... Done
--uuu-- End of apt-get output ---------------------
[INFO] Running apt-get install
--nnn-- Start of apt-get output -------------------
Reading package lists... Done
Building dependency tree       
Reading state information... Done
gdebi-core is already the newest version (0.9.5.7+nmu3).
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
--uuu-- End of apt-get output ---------------------
[INFO] Running gdebi
--nnn-- Start of gdebi output ---------------------
Reading package lists... Done
Building dependency tree        
Reading state information... Done
Reading state information... Done
Selecting previously unselected package r-4.2.0.
(Reading database ... 276977 files and directories currently installed.)
Preparing to unpack /tmp/rig/r-4.2.0_1_amd64.deb ...
Unpacking r-4.2.0 (1) ...
Setting up r-4.2.0 (1) ...
--uuu-- End of gdebi output -----------------------
[INFO] Adding /usr/local/bin/R-4.2.0 -&amp;gt; /opt/R/4.2.0/bin/R
[INFO] Setting default CRAN mirror
[INFO] Setting up RSPM
[INFO] Setting up automatic system requirements installation.
[INFO] Installing pak for R 4.2.0 (if not installed yet)
/opt/R/4.2.0/bin/R: line 193: /usr/bin/sed: No such file or directory
ERROR: option &#39;-e&#39; requires a non-empty argument
[ERROR] Failed to run R 4.2.0 to install pak
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;How come! The ubiquitous &lt;code&gt;sed&lt;/code&gt; is missing!&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(base) luolab@luolab-Z10PE-D16-WS:~$ which sed
/bin/sed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It turns out that &lt;code&gt;sed&lt;/code&gt; is located at &lt;code&gt;/bin/sed&lt;/code&gt; after Ubuntu upgrade whereas R still looks for it at &lt;code&gt;/usr/bin/sed&lt;/code&gt;.
To fix that, we have to use &lt;code&gt;sudo&lt;/code&gt; power.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(base) luolab@luolab-Z10PE-D16-WS:~$ sudo mv /bin/sed /usr/bin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In fact I&amp;rsquo;ve had the same error (cannot find utility in &lt;code&gt;/usr/bin&lt;/code&gt; because it&amp;rsquo;s located in &lt;code&gt;/bin&lt;/code&gt;) with multiple package installations, such as &lt;code&gt;tar&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The above method fix the issue just fine.&lt;/p&gt;
- /post/fixing-a-bug-in-r-after-ubuntu-upgrade/ - </description>
        </item>
    
    
    
        <item>
        <title>Switch R Faster</title>
        <link>/post/switch-r-faster/</link>
        <pubDate>Wed, 15 Jun 2022 20:52:38 +0800</pubDate>
        
        <guid>/post/switch-r-faster/</guid>
        <description>Karmotrine Dream /post/switch-r-faster/ -&lt;p&gt;Previously I wrote about how to build multiple R versions on Ubuntu and how to switch versions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ruiyuraywang.github.io/post/mult-r/&#34;&gt;R：同时部署多版本 &amp;amp; 版本切换&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It turns out that my &amp;ldquo;old school&amp;rdquo; move (&lt;code&gt;ln -s&lt;/code&gt;) is outdated. Now there are many handy tools to do the job for you.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rud.is/rswitch/&#34;&gt;RSwitch&lt;/a&gt; is based on macOS. I&amp;rsquo;m using it personally, simply because I used it before and don&amp;rsquo;t want to bother changing gear.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r-lib/rig&#34;&gt;rig&lt;/a&gt; supports multiple platforms and is probably the best choice for most users.&lt;/p&gt;
&lt;p&gt;Happy R swithing!&lt;/p&gt;
- /post/switch-r-faster/ - </description>
        </item>
    
    
    
        <item>
        <title>The Bitter Lesson</title>
        <link>/post/the_bitter_lesson/</link>
        <pubDate>Fri, 27 May 2022 09:21:08 +0800</pubDate>
        
        <guid>/post/the_bitter_lesson/</guid>
        <description>Karmotrine Dream /post/the_bitter_lesson/ -&lt;p&gt;Richard Sutton, the reknowned computer scientist, author of the AI bible: &amp;lsquo;Reinforcement Learning&amp;rsquo;, wrote this essay three years after Alphago beat Lee Sedol.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;../../figs/the_bitter_lesson.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34;&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here are some excerptions from the article:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. &amp;hellip; These researchers wanted methods based on human input to win and were disappointed when they did not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;A similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. &amp;hellip; In computer Go, as in computer chess, researchers&#39; initial effort was directed towards utilizing human understanding (so that less search was needed) and only much later was much greater success had by embracing search and learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. &amp;hellip; As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds worked&amp;mdash;they tried to put that knowledge in their systems&amp;mdash;but it proved ultimately counterproductive, and a colossal waste of researcher&amp;rsquo;s time, when, through Moore&amp;rsquo;s law, massive computation became available and a means was found to put it to good use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. &lt;strong&gt;We have to learn the bitter lesson that building in how we think we think does not work in the long run.&lt;/strong&gt; The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are &lt;em&gt;search&lt;/em&gt; and &lt;em&gt;learning&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The second general point to be learned from the bitter lesson is that &lt;strong&gt;the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds&lt;/strong&gt;, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. &lt;strong&gt;We want AI agents that can discover like we can, not which contain what we have discovered.&lt;/strong&gt; Building in our discoveries only makes it harder to see how the discovering process can be done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These points seem relevant not just to the fields of AI, but to other fields of science as well.&lt;/p&gt;
- /post/the_bitter_lesson/ - </description>
        </item>
    
    
    
        <item>
        <title>多重比较与P值矫正(一)：FWER</title>
        <link>/post/p_correction_fdr/</link>
        <pubDate>Sun, 27 Feb 2022 22:57:36 +0800</pubDate>
        
        <guid>/post/p_correction_fdr/</guid>
        <description>Karmotrine Dream /post/p_correction_fdr/ -&lt;p&gt;随着高通量技术逐渐发展完善，现代生物学的组学实验已经可以实现同时测量成百上千甚至上万个变量（variables）。如一次转录组实验可以得到10000-20000个基因的转录本丰度，蛋白组实验可以得到几千个有效蛋白肽段的丰度，磷酸化蛋白组可以得到~10000个磷酸化位点的肽段丰度。一组实验中我们往往会设置至少两个条件，而&lt;/p&gt;
- /post/p_correction_fdr/ - </description>
        </item>
    
    
    
        <item>
        <title>Tidy Data</title>
        <link>/post/tidy_data/</link>
        <pubDate>Mon, 21 Feb 2022 18:56:53 +0800</pubDate>
        
        <guid>/post/tidy_data/</guid>
        <description>Karmotrine Dream /post/tidy_data/ -&lt;figure&gt;&lt;img src=&#34;../../figs/tidydata.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Hadley Wickham提出的Tidy Data的概念方兴未艾，如今无论是R还是Python都能见到Tidy的身影。Tidydata的思维方式，将来必定会越来越深入人心。&lt;/p&gt;
&lt;p&gt;正如进入21世纪，计算机基本操作逐渐变为一项几乎必修的技能一样，将来十几年内，Tidydata对DataFrame的操作也一定会演变成一项必修技能。&lt;/p&gt;
&lt;p&gt;如果要开一个&lt;code&gt;计算生物学101&lt;/code&gt;的课程，&amp;ldquo;Tidy Data&amp;quot;这篇文章一定是必读清单的一篇文章。而且应该安排学生每学期花60个学时以上的时间上机实操锻炼Tidydata的操作。（我真是魔鬼）&lt;/p&gt;
- /post/tidy_data/ - </description>
        </item>
    
    
    
        <item>
        <title>一图看懂单细胞文库Reads的组成成分</title>
        <link>/post/10x_reads_composition/</link>
        <pubDate>Sun, 20 Feb 2022 10:55:43 +0800</pubDate>
        
        <guid>/post/10x_reads_composition/</guid>
        <description>Karmotrine Dream /post/10x_reads_composition/ -&lt;p&gt;这篇文章是前两篇技术文档的后续：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/post/10x_calc_saturation_1/&#34;&gt;10X CellRanger 中测序饱和度的定义与计算（一）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/post/10x_calc_saturation_2/&#34;&gt;10X CellRanger 中测序饱和度的定义与计算（二）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;前两篇文章成文过程中我意识到，单细胞测序文库从Raw Data到Deduplicated Reads，其实是一个non-trivial的流程。从fastq文件下机开始，不同来源的Reads需经过层层计算和筛选，最终只有少部分Reads能保留在表达矩阵（或counts table）中。这里用一张图总结，以帮助我们更好地理解单细胞文库的建库和上游分析（Upstream Analysis）。&lt;/p&gt;
&lt;hr&gt;
&lt;figure&gt;&lt;img src=&#34;../../figs/reads_composition.png&#34;/&gt;
&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;以&lt;a href=&#34;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&#34;&gt;ScRNAseq_smkpipe_at_Luolab&lt;/a&gt;的分析pipeline为例。&lt;br&gt;
这套流程主要针对使用了Barcode + UMI的技术。&lt;/p&gt;
&lt;p&gt;流程图读法：从下往上读，不同颜色代表不同分析阶段的Reads；画斜条纹阴影的线段表示当前步骤到下一步中被清除掉的Reads。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;黑色：Total Input Reads，一般我用的是测序下机后的Clean Reads（公司给的QC质控后的Cleandata）&lt;/li&gt;
&lt;li&gt;黑色-&amp;gt;紫色：&lt;code&gt;umi_tools whitelist&lt;/code&gt;, &lt;code&gt;wash_whitelist&lt;/code&gt;, &lt;code&gt;umi_tools extract&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;黑色阴影：Reads with Non-correctable Reads, invalid Barcode &amp;amp; UMI (discarded)&lt;/li&gt;
&lt;li&gt;紫色：Barcode有效的Reads（Reads with valid Barcode &amp;amp; UMI）&lt;/li&gt;
&lt;li&gt;紫色-&amp;gt;黄色：&lt;code&gt;STAR&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;紫色阴影：Reads NOT Unique Mapped to Genome (discarded)&lt;/li&gt;
&lt;li&gt;黄色：Unique Mapped Reads，在10X中也叫Confidently Mapped Reads（Unique Mapped, valid Barcode &amp;amp; UMI Reads）&lt;/li&gt;
&lt;li&gt;黄色-&amp;gt;绿色：&lt;code&gt;featureCounts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;黄色阴影：Unique Mapped, valid Barcode &amp;amp; UMI but NOT assigned to feature&lt;/li&gt;
&lt;li&gt;绿色：Effective Reads，即Unique Mapped, valid Barcode &amp;amp; UMI, feature assigned Reads&lt;/li&gt;
&lt;li&gt;绿色-&amp;gt;蓝色：&lt;code&gt;umi_tools count&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;蓝色阴影：Reads Collapsed due to UMI deduplication&lt;/li&gt;
&lt;li&gt;蓝色：Deduplicated Reads，即最终用于生成表达矩阵的对Reads计数的结果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了这张图，我们可以做一个简单直观的推论：如果想提高单细胞测序的技术质量，则必须尽可能减少阴影部分的Reads在总文库中的比例。&lt;/p&gt;
&lt;p&gt;仔细思考实验流程，会发现每个阴影部分会受不同因素的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;黑色阴影：主要影响因素之一是Barcode &amp;amp; UMI序列质量，即TSO引物质量；也有可能受TSO concatamer、RNA降解因素影响&lt;/li&gt;
&lt;li&gt;紫色阴影：主要影响因素是样本制备质量，即细胞活率、异源RNA污染等&lt;/li&gt;
&lt;li&gt;黄色阴影：主要影响因素是生物学样本内非mRNA成分的比例，如premRNA等，可能与样本本身生物学性质有关&lt;/li&gt;
&lt;li&gt;蓝色阴影：主要与扩增循环数有关，该部分Reads过多可能是因为扩增循环数过高&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“降低阴影部分比例”可以作为单细胞测序实验的一项指导原则，帮助我们进行实验条件的摸索，也可以作为单细胞技术开发的一项指导原则，帮助我们迭代优化试剂组成。&lt;/p&gt;
- /post/10x_reads_composition/ - </description>
        </item>
    
    
    
        <item>
        <title>Hugo：启用Mathjax实现数学公式的编辑与显示</title>
        <link>/post/deploy_mathjax/</link>
        <pubDate>Sat, 12 Feb 2022 21:17:37 +0800</pubDate>
        
        <guid>/post/deploy_mathjax/</guid>
        <description>Karmotrine Dream /post/deploy_mathjax/ -&lt;p&gt;The HUGO official document has a nice blog about &lt;a href=&#34;https://bwaycer.github.io/hugo_tutorial.hugo/tutorials/mathjax/&#34;&gt;MathJax Support&lt;/a&gt;.
Additionally, a blog written by Geoff Ruddock has a more concise solution: &lt;a href=&#34;https://geoffruddock.com/math-typesetting-in-hugo/&#34;&gt;Render LaTeX math expressions in Hugo with MathJax 3&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;tldr&#34;&gt;TL;DR&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Create a file in your theme directory &lt;code&gt;layouts/partials/mathjax_support.html&lt;/code&gt; as the following:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;script&amp;gt;
  MathJax = {
    tex: {
      inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\\(&#39;, &#39;\\)&#39;]],
      displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;, &#39;\\]&#39;]],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;]
    }
  };

  window.addEventListener(&#39;load&#39;, (event) =&amp;gt; {
      document.querySelectorAll(&amp;quot;mjx-container&amp;quot;).forEach(function(x){
        x.parentElement.classList += &#39;has-jax&#39;})
    });

&amp;lt;/script&amp;gt;
&amp;lt;script src=&amp;quot;https://polyfill.io/v3/polyfill.min.js?features=es6&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script type=&amp;quot;text/javascript&amp;quot; id=&amp;quot;MathJax-script&amp;quot; async
  src=&amp;quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Next, open the file &lt;code&gt;layouts/partials/header.html&lt;/code&gt; and add the following line just before the closing &lt;!-- raw HTML omitted --&gt; tag:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{{ if .Params.mathjax }}{{ partial &amp;quot;mathjax_support.html&amp;quot; . }}{{ end }}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Then, add the following lines to your CSS file. You may need to tinker with the contents here depending on your theme, these are just the settings which worked for me.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;code.has-jax {font: inherit;
              font-size: 100%;
              background: inherit;
              border: inherit;
              color: #515151;}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To modify css, refer to &lt;a href=&#34;https://www.banjocode.com/post/hugo/custom-css/&#34;&gt;Add Custom CSS Or Javascript To Your Hugo Site&lt;/a&gt;。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Finally, add &lt;code&gt;mathjax: true&lt;/code&gt; to the YAML frontmatter of any pages containing math markup. Alternatively, you could omit the outer &lt;code&gt;{{ if .Params.mathjax }} … {{ end }}&lt;/code&gt; conditional above to load the library automatically on all pages. However given that this library is quite heavy (it’s consistently the asset that Google PageSpeed Insights complains the most about) and that only &amp;lt;20% of my blog posts contain math at all, this is worth the extra effort for me.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;todo&#34;&gt;TODO&lt;/h1&gt;
&lt;p&gt;走完上面的流程后还是发现MathJax中&lt;code&gt;\color&lt;/code&gt;和&lt;code&gt;\textcolor&lt;/code&gt;不能修改字体颜色。&lt;br&gt;
custom.css似乎也没有正确配置。&lt;/p&gt;
- /post/deploy_mathjax/ - </description>
        </item>
    
    
    
        <item>
        <title>10X CellRanger 中测序饱和度的定义与计算（一）</title>
        <link>/post/10x_calc_saturation_1/</link>
        <pubDate>Tue, 08 Feb 2022 22:05:51 +0800</pubDate>
        
        <guid>/post/10x_calc_saturation_1/</guid>
        <description>Karmotrine Dream /post/10x_calc_saturation_1/ -&lt;p&gt;最近为了搭建单细胞测序上游分析的Pipeline，正在研究怎样对分析结果做质控（Pipeline详情移步&lt;a href=&#34;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&#34;&gt;https://github.com/RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab&lt;/a&gt;）。参考了10X CellRanger官方网站中描述的对测序饱和度的定义和计算方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kb.10xgenomics.com/hc/en-us/articles/115005062366-What-is-sequencing-saturation-&#34;&gt;10X 官方文档：什么是测序饱和度？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kb.10xgenomics.com/hc/en-us/articles/115003646912-How-is-sequencing-saturation-calculated-&#34;&gt;10X 官方文档：测序饱和度怎样计算？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将用两篇文章记录这部分内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：本系列文章的参考来源均为10X官方公开的信息。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这篇文章是本系列文章的第一篇，将对相关概念和计算公式的定义进行解读。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一测序饱和度-和-文库复杂度&#34;&gt;一、&lt;code&gt;测序饱和度&lt;/code&gt; 和 &lt;code&gt;文库复杂度&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;关于测序饱和度的定义，引用10X文档中的说明：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is sequencing saturation?&lt;br&gt;
&lt;strong&gt;Answer:&lt;/strong&gt; Sequencing saturation is a measure of &lt;em&gt;&lt;strong&gt;the fraction of library complexity&lt;/strong&gt;&lt;/em&gt; that was sequenced in a given experiment. The &lt;em&gt;&lt;strong&gt;inverse&lt;/strong&gt;&lt;/em&gt; of the sequencing saturation can be interpreted as the number of additional reads it would take to detect a new transcript.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这两句话言简而意赅，点明了测序饱和度(sequencing saturation)与文库复杂度(library complexity)之间的关系。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gatk.broadinstitute.org/hc/en-us/articles/360037591931-EstimateLibraryComplexity-Picard-&#34;&gt;library complexity (文库复杂度)&lt;/a&gt;，指的是一个测序文库中非冗余的DNA片段的数量（the number of unique DNA fragments present in a given library）。&lt;br&gt;
&lt;strong&gt;&lt;code&gt;测序饱和度&lt;/code&gt;表征了当前的测序数据中测出了多少&lt;code&gt;文库复杂度&lt;/code&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;从逆命题的角度来看待这个问题：发明一个新名词&lt;code&gt;“测序不饱和度”&lt;/code&gt;，则&lt;code&gt;“测序不饱和度”&lt;/code&gt;可以定义为 &lt;em&gt;再多测序多少个Reads就可以检测到一个全新的转录本&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;。&lt;br&gt;
换句话说，&lt;strong&gt;如果我们只需要再测很少的Reads就可以得到一个全新的转录本，说明这个文库“测序不饱和”；反之，如果需要再测很多Reads才能得到一个全新的转录本，说明这个文库“测序饱和”。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其实我们还可以从&lt;strong&gt;冗余信息&lt;/strong&gt;的角度来理解这个问题：&lt;br&gt;
对于一个文库，&lt;strong&gt;如果仅再测很少Reads就检测到了全新的转录本，说明当前数据还远未挖掘出文库中的大部分信息，数据的信息冗余程度比较低，测序不饱和；&lt;/strong&gt;&lt;br&gt;
而反过来，&lt;strong&gt;如果需要再测很多Reads才能检测到全新的转录本，说明当前数据对文库中包含的信息挖掘得比较充分，数据的信息冗余程度比较高，测序接近饱和。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以本质上，测序饱和度可以间接地由&lt;strong&gt;信息冗余程度&lt;/strong&gt;进行度量。&lt;br&gt;
数据信息冗余度越高，测序饱和度越高；&lt;br&gt;
数据信息冗余度越低，测序饱和度越低。&lt;/p&gt;
&lt;p&gt;从实际角度出发，我们有时候&lt;strong&gt;并不希望信息冗余度&lt;/strong&gt;太高。因为冗余信息越多，浪费的资源越多。所以有些研究不会将 测序饱和度 的目标阈值设定到接近饱和（&amp;gt;90%），而是测到70%左右就收工了。&lt;br&gt;
在受经费条件等因素限制的情况下这是一种合理策略。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二影响测序饱和度的因素&#34;&gt;二、影响测序饱和度的因素&lt;/h3&gt;
&lt;p&gt;在单细胞测序中，测序饱和度主要由两个因素决定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;文库复杂度(Biological Complxity)：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Different cell types will have different amounts of RNA and thus will differ in the total number of different transcripts in the final library (also known as library complexity). The figure below illustrates the median number of genes recovered from different cell types. As sequencing depth increases, more genes are detected, but this reaches saturation at different sequencing depths depending on cell type.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;文库复杂度一般由生物学样本的复杂程度决定。而生物学样本的复杂度又由多个因素，如细胞异质性(Cell Heterogeneity)、转录组RNA含量、细胞质量、细胞数等共同决定。&lt;del&gt;越是简单均一的系统，生物复杂程度越低。&lt;/del&gt;&lt;br&gt;
下图中，原代分离的PBMC(外周血单核细胞)和E18小鼠神经元细胞RNA含量较低（？）&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，因此迅速达到测序饱和；而培养的永生细胞3T3和HEK293T的RNA含量比较高（？）&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，测序不容易达到饱和。同样作为永生细胞，3T3细胞系的转录组比HEK293T细胞系的转录组复杂，因此相比之下不容易测序饱和。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;测序深度(Sequencing Depth)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sequencing depth also affects sequencing saturation; generally, the more sequencing reads, the more additional unique transcripts you can detect. However, this is limited by the library complexity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;即每个文库实际测到的Reads数。在控制其他条件不变的情况下，一般来讲测序深度越高测序饱和度越高，直观上这很容易理解。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kb.10xgenomics.com/hc/article_attachments/115016412246/SingleCell_GeneRecovery_by_SeqDepth_combined.png#middle&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;上面的内容可以总结在一个公式中：&lt;/p&gt;
&lt;p&gt;$$测序饱和度\propto\frac{测序深度}{文库复杂度}$$&lt;/p&gt;
&lt;p&gt;如果一个样本的生物学成分比较复杂，那送样时就可以提前计划适当提高测序深度；如果测完发现测序饱和度还是不够，那就对文库进行加测（提高测序深度）。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;三测序饱和度的计算&#34;&gt;三、测序饱和度的计算&lt;/h3&gt;
&lt;p&gt;10X官方文档给出了测序饱和度计算公式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$Sequencing\ Saturation = 1 - \frac{n_{deduped\ reads}}{n_{reads}}$$&lt;br&gt;
where&lt;br&gt;
$n_{deduped\ reads}$ = Number of unique (valid cell-barcode, valid UMI, gene) combinations among confidently mapped reads&lt;br&gt;
$n_{reads}$ = Total number of confidently mapped, valid cell-barcode, valid UMI reads&lt;br&gt;
.&lt;br&gt;
Note that the numerator of the fraction is $n_{deduped\ reads}$, not the non-unique reads that are mentioned in the definition. $n_{deduped\ reads}$ is a degree of uniqueness, not a degree of duplication/saturation. Therefore we take the complement of $n_{deduped\ reads}/n_{reads}$ to measure saturation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;即&lt;/p&gt;
&lt;p&gt;$$测序饱和度=1-\frac{去重后Reads数}{总Reads数}=\frac{重复Reads数}{总Reads数}$$&lt;/p&gt;
&lt;p&gt;注意重复Reads是通过Barcode和UMI定义的。两个Reads出现同样的Barcode + UMI组合，就将其中一个定义为重复；三个Reads出现同样的Barcode + UMI组合，其中两个定义为重复，剩下一个定义为去重后Read。&lt;/p&gt;
&lt;p&gt;从这个公式也可以看出，重复Reads数越高，冗余信息越多，测序饱和度也越高。&lt;/p&gt;
&lt;p&gt;该公式只计算了一个数值，代表某次测序fastq文件整个数据的测序饱和度。如果要实现10X CellRanger输出web_summary的中饱和度曲线的绘制，感觉还是比较麻烦的。理论上，需要对fastq文件做不同水平的降采样，再分别算测序饱和度。&lt;br&gt;
目前还不清楚CellRanger是怎么实现一次alignment过程中输出不同测序深度下的测序饱和度的。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;全新的转录本，在10X官方文档的语境下，指的是一个全新的 Barcode + UMI 组合（i.e. a previously unobserved, unique combination of valid cell-barcode and valid UMI）&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;直觉上这并不 make sense。原代细胞有较高的细胞异质性，如果其他条件相同，理应更难达到测序饱和。猜测也许是因为原代细胞分离下来细胞状态不如永生细胞，因此导致文库质量不佳。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
- /post/10x_calc_saturation_1/ - </description>
        </item>
    
    
  </channel>
</rss> 